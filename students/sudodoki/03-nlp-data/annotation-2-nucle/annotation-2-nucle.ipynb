{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Process data from the [NUCLE Error Corpus](http://www.comp.nus.edu.sg/~nlp/conll14st.html#nucle32) and analyze inter-annotator agreement in it (general and for each error type). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"conll14st-test-data.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = tar.getmembers()[12] # conll14st-test-data/noalt/official-2014.0.sgml\n",
    "set2 = tar.getmembers()[14] # conll14st-test-data/noalt/official-2014.1.sgml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def debug_events(parser):\n",
    "    for action, element in parser.read_events():\n",
    "        print('%s: %s %s %s' % (action, element.tag, element.attrib, element.text))\n",
    "\n",
    "def file_to_annotation_docs(fileObj):\n",
    "    parser = etree.XMLPullParser(events=('start', 'end'))    \n",
    "    docs = []\n",
    "    currentDoc = None\n",
    "    currentAnnotation = None\n",
    "    currentMistake = None\n",
    "    currentText = ''\n",
    "    parser.feed(b'<EVERYTHING>\\n')\n",
    "    for line in fileObj.readlines():        \n",
    "        parser.feed(line)\n",
    "        for action, element in parser.read_events():\n",
    "            if (action == 'start' and element.tag == 'DOC'):\n",
    "                currentDoc = {\n",
    "                    'currentText': '',\n",
    "                    'nid': element.attrib['nid']\n",
    "                }\n",
    "                currentText = ''\n",
    "                continue\n",
    "            if (action == 'end' and element.tag == 'DOC'):\n",
    "                currentDoc['currentText'] = currentText\n",
    "                docs.append(currentDoc)\n",
    "                currentDoc = None\n",
    "                continue\n",
    "            if (action == 'start' and element.tag == 'TEXT'):\n",
    "                continue\n",
    "            if (action == 'end' and element.tag in ['TITLE', 'P']):\n",
    "                currentText += element.text\n",
    "                continue\n",
    "            if (action == 'start' and element.tag == 'ANNOTATION'):\n",
    "                currentAnnotation = {\n",
    "                    'teacher_id': element.attrib['teacher_id'],\n",
    "                    'mistakes': []\n",
    "                }\n",
    "                currentDoc['annotation'] = currentAnnotation\n",
    "                continue\n",
    "            if (action == 'start' and element.tag == 'MISTAKE'):\n",
    "                currentMistake = {\n",
    "                    'start': str(int(element.attrib['start_par']) * 1000) + element.attrib['start_off'],\n",
    "                    'end': str(int(element.attrib['end_par']) * 1000) + element.attrib['end_off']\n",
    "                }\n",
    "                continue\n",
    "            if (action == 'end' and element.tag == 'MISTAKE'):\n",
    "                currentAnnotation['mistakes'].append(currentMistake)\n",
    "                continue\n",
    "            if (action == 'end' and element.tag == 'TYPE'):\n",
    "                currentMistake['type'] = element.text\n",
    "                continue\n",
    "            if (action == 'end' and element.tag == 'CORRECTION'):\n",
    "                currentMistake['corr'] = element.text\n",
    "                continue         \n",
    "    parser.feed(b'</EVERYTHING>\\n')\n",
    "    \n",
    "    return docs \n",
    "docset1 = file_to_annotation_docs(tar.extractfile(set1))\n",
    "docset2 = file_to_annotation_docs(tar.extractfile(set2))\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1'),\n",
       " ('2', '2'),\n",
       " ('3', '3'),\n",
       " ('4', '4'),\n",
       " ('5', '5'),\n",
       " ('6', '6'),\n",
       " ('7', '7'),\n",
       " ('8', '8'),\n",
       " ('9', '9'),\n",
       " ('10', '10'),\n",
       " ('11', '11'),\n",
       " ('12', '12'),\n",
       " ('13', '13'),\n",
       " ('14', '14'),\n",
       " ('15', '15'),\n",
       " ('16', '16'),\n",
       " ('17', '17'),\n",
       " ('18', '18'),\n",
       " ('19', '19'),\n",
       " ('20', '20'),\n",
       " ('21', '21'),\n",
       " ('22', '22'),\n",
       " ('23', '23'),\n",
       " ('24', '24'),\n",
       " ('25', '25'),\n",
       " ('26', '26'),\n",
       " ('27', '27'),\n",
       " ('28', '28'),\n",
       " ('29', '29'),\n",
       " ('30', '30'),\n",
       " ('31', '31'),\n",
       " ('32', '32'),\n",
       " ('33', '33'),\n",
       " ('34', '34'),\n",
       " ('35', '35'),\n",
       " ('36', '36'),\n",
       " ('37', '37'),\n",
       " ('38', '38'),\n",
       " ('39', '39'),\n",
       " ('40', '40'),\n",
       " ('41', '41'),\n",
       " ('42', '42'),\n",
       " ('43', '43'),\n",
       " ('44', '44'),\n",
       " ('45', '45'),\n",
       " ('46', '46'),\n",
       " ('47', '47'),\n",
       " ('48', '48'),\n",
       " ('49', '49'),\n",
       " ('50', '50')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(doc1['nid'], doc2['nid']) for doc1, doc2 in zip(docset1, docset2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_equal(loc1, loc2):\n",
    "    return loc1['start'] == loc2['start'] and loc1['end'] == loc2['end']\n",
    "\n",
    "def location_includes(loc1, loc2):\n",
    "    return int(loc1['start']) >= int(loc2['start']) and int(loc1['end']) <= int(loc2['end']) or  int(loc2['start']) >= int(loc1['start']) and int(loc2['end']) <= int(loc1['end'])\n",
    "\n",
    "def key_equals(key):\n",
    "    return lambda obj1, obj2: obj1[key] == obj2[key]\n",
    "def hashable_error(e, include_type=False, include_corr=False):\n",
    "    res = {\n",
    "        'start': e['start'],\n",
    "        'end': e['end']\n",
    "    }\n",
    "    if include_type:\n",
    "        res['type'] = e['type']\n",
    "    if include_corr:\n",
    "        res['corr'] = e['corr']\n",
    "    return frozenset(res.items())\n",
    "\n",
    "def check_agreement(error_set1, error_set2, include_type=False, include_corr=False, location_check=location_equal):\n",
    "    mistakes1, mistakes2 = error_set1['mistakes'], error_set2['mistakes']\n",
    "    intersection = set()\n",
    "    union = set()\n",
    "    for mistake_set in [mistakes1, mistakes2]:\n",
    "        for mistake in mistake_set:\n",
    "            same = [other for other in mistakes2 if location_check(mistake, other)]\n",
    "            different = []\n",
    "            if include_type:\n",
    "                new_same = [other for other in same if key_equals('type')(mistake, other)]\n",
    "                new_different = [other for other in same if not key_equals('type')(mistake, other)]\n",
    "                same = new_same\n",
    "                different = new_different\n",
    "            if include_corr:\n",
    "                new_same = [other for other in same if key_equals('corr')(mistake, other)]\n",
    "                new_different = [other for other in same if not key_equals('corr')(mistake, other)]\n",
    "                same = new_same\n",
    "                different = different + new_different\n",
    "            for e in same:\n",
    "                intersection.add(hashable_error(e, include_type, include_corr))\n",
    "            union.add(hashable_error(mistake, include_type, include_corr))\n",
    "            for e in different:\n",
    "                union.add(hashable_error(e, include_type, include_corr))\n",
    "    return intersection, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agremeent with exactly same location  same type  same correction  agremenet is  65.75207264113699 %\n",
      "Agremeent with location included in other location  same type  same correction  agremenet is  65.75207264113699 %\n",
      "Agremeent with exactly same location  same type  ignoring correction  agremenet is  67.77212614445574 %\n",
      "Agremeent with location included in other location  same type  ignoring correction  agremenet is  67.77212614445574 %\n",
      "Agremeent with exactly same location  ignoring types  same correction  agremenet is  67.89645332246229 %\n",
      "Agremeent with location included in other location  ignoring types  same correction  agremenet is  67.89645332246229 %\n",
      "Agremeent with exactly same location  ignoring types  ignoring correction  agremenet is  72.95225580376697 %\n",
      "Agremeent with location included in other location  ignoring types  ignoring correction  agremenet is  72.95225580376697 %\n"
     ]
    }
   ],
   "source": [
    "def get_agreement(docset1, docset2, include_type=False, include_corr=False, location_check=location_equal):\n",
    "    total_interesction = 0\n",
    "    total_union = 0\n",
    "    for doc1, doc2 in zip(docset1, docset2):\n",
    "        intersection, union = check_agreement(doc1['annotation'], doc2['annotation'], include_type, include_corr, location_check)\n",
    "        total_interesction += len(intersection)\n",
    "        total_union += len(union)\n",
    "    agreement = total_interesction / total_union * 100\n",
    "    return agreement\n",
    "\n",
    "for include_type in (True, False):\n",
    "    for include_corr in (True, False):\n",
    "        for location_check in (location_equal, location_includes):\n",
    "            banner = 'Agremeent with'\n",
    "            if location_check == location_equal:\n",
    "                banner += ' exactly same location '\n",
    "            else:\n",
    "                banner += ' location included in other location '\n",
    "                \n",
    "            banner += ' same type ' if include_type else ' ignoring types '\n",
    "            banner += ' same correction ' if include_corr else ' ignoring correction '\n",
    "            banner += ' agremenet is '\n",
    "            print(banner, get_agreement(docset1, docset2, include_type=include_type, include_corr=include_corr, location_check=location_check), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "- the stricter criteria of agreements, the less agreement there is\n",
    "- there's only 65% of cases of total inter-annotator agreement with exact match of location type and correction\n",
    "- in 2% of corrections for same error and location annotators came up with different corrections\n",
    "- in 72% of cases correction of one person would somewhat overlap other person correction, disregarding other error details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
