{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1100\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">stripline</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VBZ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">shrouded</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VBN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">by</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">fragments</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NNS</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'The stripline is shrouded by fragments')\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лес\n",
      "\n",
      "Porter: лес\n",
      "Snowball: лес\n",
      "Lancaster: лес\n",
      "\n",
      "лесной\n",
      "\n",
      "Porter: лесной\n",
      "Snowball: лесн\n",
      "Lancaster: лесной\n",
      "\n",
      "лесник\n",
      "\n",
      "Porter: лесник\n",
      "Snowball: лесник\n",
      "Lancaster: лесник\n",
      "\n",
      "лесничий\n",
      "\n",
      "Porter: лесничий\n",
      "Snowball: леснич\n",
      "Lancaster: лесничий\n",
      "\n",
      "лесничество\n",
      "\n",
      "Porter: лесничество\n",
      "Snowball: лесничеств\n",
      "Lancaster: лесничество\n",
      "\n",
      "пролесье\n",
      "\n",
      "Porter: пролесье\n",
      "Snowball: пролес\n",
      "Lancaster: пролесье\n",
      "\n",
      "flaw\n",
      "\n",
      "Porter: flaw\n",
      "Snowball: flaw\n",
      "Lancaster: flaw\n",
      "\n",
      "flaws\n",
      "\n",
      "Porter: flaw\n",
      "Snowball: flaws\n",
      "Lancaster: flaw\n",
      "\n",
      "flawed\n",
      "\n",
      "Porter: flaw\n",
      "Snowball: flawed\n",
      "Lancaster: flaw\n",
      "\n",
      "flawless\n",
      "\n",
      "Porter: flawless\n",
      "Snowball: flawless\n",
      "Lancaster: flawless\n",
      "\n",
      "flawlessness\n",
      "\n",
      "Porter: flawless\n",
      "Snowball: flawlessness\n",
      "Lancaster: flawless\n",
      "\n",
      "flawlessly\n",
      "\n",
      "Porter: flawlessli\n",
      "Snowball: flawlessl\n",
      "Lancaster: flawless\n",
      "\n",
      "flawness\n",
      "\n",
      "Porter: flaw\n",
      "Snowball: flawness\n",
      "Lancaster: flaw\n",
      "\n",
      "окно\n",
      "\n",
      "Porter: окно\n",
      "Snowball: окн\n",
      "Lancaster: окно\n",
      "\n",
      "окошко\n",
      "\n",
      "Porter: окошко\n",
      "Snowball: окошк\n",
      "Lancaster: окошко\n",
      "\n",
      "подоконник\n",
      "\n",
      "Porter: подоконник\n",
      "Snowball: подоконник\n",
      "Lancaster: подоконник\n",
      "\n",
      "оконный\n",
      "\n",
      "Porter: оконный\n",
      "Snowball: окон\n",
      "Lancaster: оконный\n",
      "\n",
      "окнище\n",
      "\n",
      "Porter: окнище\n",
      "Snowball: окнищ\n",
      "Lancaster: окнище\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [\"лес\", \"лесной\", \"лесник\", \"лесничий\", \"лесничество\", \"пролесье\", \n",
    "         \"flaw\", \"flaws\", \"flawed\", \"flawless\", \"flawlessness\", \"flawlessly\", \"flawness\", \n",
    "         \"окно\", \"окошко\", \"подоконник\", \"оконный\", \"окнище\"]\n",
    "p_stemmer = PorterStemmer()\n",
    "s_stemmer = SnowballStemmer(\"russian\")\n",
    "l_stemmer = LancasterStemmer()\n",
    "for word in words:\n",
    "    print(f\"{word}\\n\\nPorter: {p_stemmer.stem(word)}\\nSnowball: {s_stemmer.stem(word)}\\nLancaster: {l_stemmer.stem(word)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import regex as re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "infixes = (\n",
    " '\\\\.\\\\.+',\n",
    " '…',\n",
    " '[\\\\p{So}]',\n",
    " '(?<=[0-9])[+\\\\\\*^](?=[0-9])',\n",
    " '(?<=[[[\\\\p{Ll}&&\\\\p{Latin}]||[ёа-я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]])\\\\.(?=[[[\\\\p{Lu}&&\\\\p{Latin}]||[ЁА-Я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]])',\n",
    " '(?<=[[[\\\\p{Lu}&&\\\\p{Latin}]||[ЁА-Я]||[\\\\p{Ll}&&\\\\p{Latin}]||[ёа-я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]]),(?=[[[\\\\p{Lu}&&\\\\p{Latin}]||[ЁА-Я]||[\\\\p{Ll}&&\\\\p{Latin}]||[ёа-я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]])',\n",
    " '(?<=[[[\\\\p{Lu}&&\\\\p{Latin}]||[ЁА-Я]||[\\\\p{Ll}&&\\\\p{Latin}]||[ёа-я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]])[?\";:=,.]*(?:–|—|--|---|——|~)(?=[[[\\\\p{Lu}&&\\\\p{Latin}]||[ЁА-Я]||[\\\\p{Ll}&&\\\\p{Latin}]||[ёа-я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]])',\n",
    " '(?<=[[[\\\\p{Lu}&&\\\\p{Latin}]||[ЁА-Я]||[\\\\p{Ll}&&\\\\p{Latin}]||[ёа-я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]\"])[:<>=/](?=[[[\\\\p{Lu}&&\\\\p{Latin}]||[ЁА-Я]||[\\\\p{Ll}&&\\\\p{Latin}]||[ёа-я]||[\\\\p{L}&&\\\\p{Bengali}]||[\\\\p{L}&&\\\\p{Hebrew}]||[\\\\p{L}&&\\\\p{Arabic}]]])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "infix_re = spacy.util.compile_infix_regex(infixes) #re.compile(r'''[~]''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     infix_finditer=infix_re.finditer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPS = [\"NOUN\", \"PRON\", \"PROPN\", \"ADJ\", \"ADV\", \"VERB\", \"SCONJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_headline(text):\n",
    "    doc = nlp(text)\n",
    "    output = []\n",
    "    for token in doc:\n",
    "        if \"-\" in token.text:\n",
    "            parts = token.text_with_ws.split(\"-\")\n",
    "            word = '-'.join(map(lambda x: x.capitalize(), parts))\n",
    "            output.append(word)\n",
    "        else:\n",
    "            if token.pos_ in CAPS:\n",
    "                output.append(token.text_with_ws.capitalize())\n",
    "            else:\n",
    "                output.append(token.text_with_ws.lower())\n",
    "    output[0] = output[0].capitalize()\n",
    "    output[-1] = output[-1].capitalize()\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Back to School, Gluten-Free Style'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_headline(\"Back to school, gluten-free style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "path = Path('/home/karimlulu/repos/prj-nlp') / \"tasks\" / \"02-structural-linguistics\"\n",
    "filename = \"examiner-headlines.txt\"\n",
    "data = [line.strip() for line in (path / filename).open().readlines()]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Pamper Yourself like a Pharaoh at the Ritz Carlton Denver\"\n",
    "format_headline(s)==s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0\n",
      "Processed: 500\n",
      "Processed: 1000\n",
      "Processed: 1500\n",
      "Processed: 2000\n",
      "Processed: 2500\n",
      "Processed: 3000\n",
      "Processed: 3500\n",
      "Processed: 4000\n",
      "Processed: 4500\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,line in enumerate(data):\n",
    "    if line==format_headline(line):\n",
    "        i += 1\n",
    "    if k%500==0:\n",
    "        print(f\"Processed: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prominence(doc):\n",
    "    return doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"NOUN\": \"n\", \n",
    "           \"VERB\": \"v\", \n",
    "           \"ADJ\": \"a\",\n",
    "           \"ADV\": \"r\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE = [\"PUNCT\", \"SYM\", \"SPACE\", \"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(doc, top=5, threshold=0.5):\n",
    "    mean_pos_sentiment = 0\n",
    "    for k, token in enumerate(doc):\n",
    "        if token.pos_ not in EXCLUDE:\n",
    "            synsets = list(swn.senti_synsets(token.text, pos=mapping.get(token.pos_)))[:top]\n",
    "            token_pos_sentiment = sum(synset.pos_score() for synset in synsets) / len(synsets) if synsets else 0\n",
    "            mean_pos_sentiment = (k * mean_pos_sentiment + token_pos_sentiment) / (k + 1)\n",
    "    return mean_pos_sentiment >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superlativeness(doc):\n",
    "    output = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "            if token.tag_ in [\"JJS\", \"RBS\"]:\n",
    "                output += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_catchy(text, top=5, threshold=0.5):\n",
    "    doc = nlp(text)\n",
    "    ner, pos_sentiment, superlat = prominence(doc), sentiment(doc, top=top, threshold=threshold), superlativeness(doc)\n",
    "    if ner or pos_sentiment or superlat:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BASIC, BABY, CHECKLIST]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0, (), False)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"BASIC BABY CHECKLIST\"\n",
    "doc = nlp(word)\n",
    "print(list(doc))\n",
    "sentiment(doc), superlativeness(doc), prominence(doc), check_catchy(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halep enters Rogers Cup final in straight sets win over Errani True\n",
      "Processed: 0\n",
      "The phantoms of St. Mary's True\n",
      "Talladega turmoil could spell trouble for NASCAR's Chase field True\n",
      "Burn those calories! Try the Very Steep Trail. False\n",
      "It's the end of the world... and I feel fine False\n",
      "2011-2012 NHL team preview: Detroit Red Wings True\n",
      "Cal coach Jeff Tedford taking a different approach in 2010 -- Part 1 True\n",
      "Google science fair to encourage STEM learning True\n",
      "SF Beer Week 2013: what's for dinner (part 2) True\n",
      "Jersey Shore Season 6 cast's salaries revealed; More than President Obama! True\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,line in enumerate(data[:10]):\n",
    "    is_catchy = check_catchy(line)\n",
    "    print(line, is_catchy)\n",
    "    i += is_catchy\n",
    "    if k%500==0:\n",
    "        print(f\"Processed: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = {\"NOUN\": \"n\", \n",
    "           \"VERB\": \"v\", \n",
    "           \"ADJ\": \"a\",\n",
    "           \"ADV\": \"r\"}\n",
    "EXCLUDE = [\"PUNCT\", \"SYM\", \"SPACE\", \"X\"]\n",
    "\n",
    "def prominence(doc):\n",
    "    return doc.ents\n",
    "\n",
    "def sentiment(doc, top=5, threshold=0.5):\n",
    "    mean_pos_sentiment = 0\n",
    "    for k, token in enumerate(doc):\n",
    "        if token.pos_ not in EXCLUDE:\n",
    "            synsets = list(swn.senti_synsets(token.text, pos=MAPPING.get(token.pos_)))[:top]\n",
    "            token_pos_sentiment = sum(synset.pos_score() for synset in synsets) / len(synsets) if synsets else 0\n",
    "            mean_pos_sentiment = (k * mean_pos_sentiment + token_pos_sentiment) / (k + 1)\n",
    "    return mean_pos_sentiment >= threshold\n",
    "\n",
    "def superlativeness(doc):\n",
    "    output = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "            if token.tag_ in [\"JJS\", \"RBS\"]:\n",
    "                output += 1\n",
    "    return output\n",
    "\n",
    "def check_catchy(text, top=5, threshold=0.5):\n",
    "    doc = nlp(text)\n",
    "    ner = prominence(doc)\n",
    "    pos_sentiment = sentiment(doc, top=top, threshold=threshold)\n",
    "    superlat = superlativeness(doc)\n",
    "    if ner or pos_sentiment or superlat:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')output.append(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_catchy(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from functools import reduce\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = [\"say\", \"tell\", \"speak\", \"claim\", \"communicate\",\n",
    "         \"narrate\", \"declare\", \"respond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[\"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"blog2008.txt\"\n",
    "with (path / filename).open() as f:\n",
    "    data = f.readlines()\n",
    "    data = [line.strip() for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verb(doc, verbs=WORDS, patt=\"ly\"):\n",
    "    output = defaultdict(list)\n",
    "    for token in doc:\n",
    "        if token.lemma_ in verbs and token.pos_ == \"VERB\":\n",
    "            adverbs = filter(lambda x: x.pos_==\"ADV\" and x.text.endswith(patt), token.children)\n",
    "            output[token.lemma_].extend([el.text.lower() for el in adverbs])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(a, b):\n",
    "    for key, value in b.items():\n",
    "        a[key].extend(value)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez = (find_verb(doc) for doc in nlp.pipe(data[:10000], batch_size=20000, n_threads=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent: 45.36\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "rez = reduce(merge_dicts, rez)\n",
    "print(f\"Spent: {time()-t0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['continually'], ['definitively', 'recently', 'absolutely', 'really', 'Really'], [], ['initially'], [], ['overtly', 'subsequently'], []\""
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(str(el) for el in output.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "frq_collocations = {}\n",
    "for key, value in rez.items():\n",
    "    frq_collocations[key] = Counter(value).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('initially', 1), ('brazenly', 1), ('surely', 1)\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(str(el) for el in frq_collocations[\"claim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0\n",
      "Processed: 1000\n",
      "Spent: 28.65\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "output = defaultdict(list)\n",
    "for k, line in enumerate(data[:2000]):\n",
    "    doc = nlp(line.strip())\n",
    "    output = find_verb(doc, output=output)\n",
    "    if k % 1000 == 0:\n",
    "        print(f\"Processed: {k}\")\n",
    "print(f\"Spent: {time()-t0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303994"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'claim': ['initially'],\n",
       "             'communicate': [],\n",
       "             'declare': [],\n",
       "             'respond': [],\n",
       "             'say': ['definitively',\n",
       "              'recently',\n",
       "              'absolutely',\n",
       "              'really',\n",
       "              'Really'],\n",
       "             'speak': ['overtly', 'subsequently'],\n",
       "             'tell': ['continually']})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ADV', 'speak')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = doc[1]\n",
    "list(x.children)[-1].pos_, x.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"shm\"\n",
    "vowels =  ('a', 'e', 'i', 'o', 'u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shmificate(word, prefix=PREFIX):\n",
    "    is_capitalized = word[0].isupper()\n",
    "    word = word.lower()\n",
    "    if word.startswith(PREFIX):\n",
    "        return word\n",
    "    if \"sh\" in word:\n",
    "        prefix = \"sm\"\n",
    "    i = 0\n",
    "    for letter in word:\n",
    "        if letter not in vowels:\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    prefix = prefix.capitalize() if is_capitalized else prefix\n",
    "    output = prefix + word[i:]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shmable', 'shmapple', 'shmaltz', 'Smashmont', 'shmuncate']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"table\", \"apple\", \"shmaltz\", \"Ashmont\", \"truncate\"]\n",
    "[shmificate(word) for word in words]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
