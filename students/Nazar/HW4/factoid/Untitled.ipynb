{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "statements = pd.read_csv('data/statements.csv', encoding='utf8')['statements'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fashion week in NYC is in February ',\n",
       "       'Roosevelt Field  site did Lindbergh begin his flight from in 1927',\n",
       "       'Saudi Arabia  country is the holy city of Mecca located in',\n",
       "       'the sun is  5778.0 hot',\n",
       "       'banana  flavor filling did the original Twinkies have'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "en_nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    nlp_doc = en_nlp(text)\n",
    "    for token in nlp_doc:\n",
    "        print('{0}: {1}, {2}({3})'.format(token,token.pos_,token.dep_,token.head))\n",
    "    \n",
    "    print('---------------------')\n",
    "    for ent in nlp_doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test spacy Entity Recogmition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana: NOUN, compound(filling)\n",
      " : SPACE, (banana)\n",
      "flavor: NOUN, compound(filling)\n",
      "filling: VERB, nsubj(have)\n",
      "did: VERB, advmod(filling)\n",
      "the: DET, det(Twinkies)\n",
      "original: ADJ, amod(Twinkies)\n",
      "Twinkies: PROPN, nsubj(have)\n",
      "have: VERB, ROOT(have)\n",
      "---------------------\n",
      "Twinkies ORG\n"
     ]
    }
   ],
   "source": [
    "# get NN, NNS\n",
    "get_entities(statements[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Wikification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saudi Arabia -> Saudi Arabia (score: 0.5400270819664001)\n",
      "country -> Nation state (score: 0.1376003473997116)\n",
      "holy city -> Jerusalem (score: 0.16229085624217987)\n",
      "Mecca -> Mecca (score: 0.6146639585494995)\n"
     ]
    }
   ],
   "source": [
    "import tagme\n",
    "tagme.GCUBE_TOKEN = \"c294fe63-88fa-40e1-b1d4-34b70c887b29-843339462\"\n",
    "for item in tagme.annotate(statements[2]).get_annotations(0.1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understend what is the connection between entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wikification of entities\n",
    "# try find something in Wiki/DBPedia\n",
    "# set statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saudi Arabia  country is the holy city of Mecca located in'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic role labeling\n",
    "allennlp\n",
    "nlpnet\n",
    "senna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-c8b9a5e26754>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommands\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDEFAULT_MODELS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSemanticRoleLabelerPredictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchival\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_archive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'allennlp'"
     ]
    }
   ],
   "source": [
    "from allennlp.commands import DEFAULT_MODELS\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.service.predictors import SemanticRoleLabelerPredictor\n",
    "from allennlp.models.archival import load_archive\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "class SRLComponent(object):\n",
    "    '''\n",
    "    A SpaCy pipeline component for SRL\n",
    "    '''\n",
    "    \n",
    "    name = 'Semantic Role Labeler'\n",
    "\n",
    "    def __init__(self):\n",
    "        archive = load_archive(self._get_srl_model())\n",
    "        self.predictor = SemanticRoleLabelerPredictor.from_archive(archive, \"semantic-role-labeling\")\n",
    "        Token.set_extension('srl_arg0')\n",
    "        Token.set_extension('srl_arg1')\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        # See https://github.com/allenai/allennlp/blob/master/allennlp/service/predictors/semantic_role_labeler.py#L74\n",
    "        words = [token.text for token in doc]\n",
    "        for i, word in enumerate(doc):\n",
    "            if word.pos_ == \"VERB\":\n",
    "                verb = word.text\n",
    "                verb_labels = [0 for _ in words]\n",
    "                verb_labels[i] = 1\n",
    "                instance = self.predictor._dataset_reader.text_to_instance(doc, verb_labels)\n",
    "                output = self.predictor._model.forward_on_instance(instance, -1)\n",
    "                tags = output['tags']\n",
    "\n",
    "                # TODO: Tagging/dependencies can be done more elegant \n",
    "                if \"B-ARG0\" in tags:\n",
    "                    start = tags.index(\"B-ARG0\")\n",
    "                    end = max([i for i, x in enumerate(tags) if x == \"I-ARG0\"] + [start]) + 1\n",
    "                    word._.set(\"srl_arg0\", doc[start:end])\n",
    "\n",
    "                if \"B-ARG1\" in tags:\n",
    "                    start = tags.index(\"B-ARG1\")\n",
    "                    end = max([i for i, x in enumerate(tags) if x == \"I-ARG1\"] + [start]) + 1\n",
    "                    word._.set(\"srl_arg1\", doc[start:end])\n",
    "        \n",
    "        return doc\n",
    "    \n",
    "    def _get_srl_model(self):\n",
    "        return cached_path(DEFAULT_MODELS['semantic-role-labeling'])\n",
    "\n",
    "    \n",
    "def demo():\n",
    "    nlp = spacy.load(\"en\")\n",
    "    nlp.add_pipe(SRLComponent(), after='ner')\n",
    "    doc = nlp(\"Apple sold 1 million Plumbuses this month.\")\n",
    "    for w in doc:\n",
    "        if w.pos_ == \"VERB\":\n",
    "            print(\"('{}', '{}', '{}')\".format(w._.srl_arg0, w, w._.srl_arg1)) \n",
    "            # ('Apple', 'sold', '1 million Plumbuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "default values:\n",
      "senna path:\n",
      "  \n",
      "Dependencie parser:\n",
      " Z:\\anaconda\\envs\\py_36\\lib\\site-packages\\pntl\\\n",
      "Stanford parser clr java -cp stanford-parser.jar edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile in.parse -collapsed\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-cf26d0c5fb7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mannotator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAnnotator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mannotator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_annoations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"He created the robot and broke it after making it.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mZ:\\anaconda\\envs\\py_36\\lib\\site-packages\\pntl\\tools.py\u001b[0m in \u001b[0;36mget_annoations\u001b[1;34m(self, sentence, senna_tags, dep_parse)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msenna_tags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m             \u001b[0msenna_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_senna_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m             \u001b[0msenna_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msenna_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[0msenna_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msenna_tags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\anaconda\\envs\\py_36\\lib\\site-packages\\pntl\\tools.py\u001b[0m in \u001b[0;36mget_senna_tag\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0msenna_executable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackage_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackage_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         pipe = subprocess.Popen(senna_executable,\n\u001b[0;32m    331\u001b[0m                                 \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: ''"
     ]
    }
   ],
   "source": [
    "#senna\n",
    "from practnlptools.tools import Annotator\n",
    "annotator=Annotator()\n",
    "\n",
    "annotator.getAnnotations(\"He created the robot and broke it after making it.\")['srl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
